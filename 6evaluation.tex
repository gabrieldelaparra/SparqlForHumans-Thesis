\chapter{Evaluation}

In this research we have built an specialized inverted index to support users on dataset exploration and SPARQL query building tasks on large-scale RDF datasets. In this chapter we present an evaluation of both the indexing and querying processes described in this work. We will start by answering some questions that our readers might find are the most relevant:

\begin{itemize}
    \item How long does the indexing process take?
    \item How much space does indexing requires?
    \item How are our times compared against normal SPARQL query times?
    \item How do our approximation suggestions compare to precise results?
\end{itemize}

In order to answer these questions, we will first present the experimental settings used for this research.

\section{Experimental setup}

For this work, we have used a personal computer with a i7-4600M CPU @ 2.9GHz and 16GB of RAM, running on Windows 10.

We based our development on the Wikidata dump of March 4. 2018 (20,821,216,963 bytes); and our evaluation of the system with the dump of May XX 2020 (XXXX bytes). We had some original results from our first Wikidata dump, but we found out that the information in our dataset was outdated enough, that results against the Wikidata query service were too different to compare.

Regarding the source code, it was written in C\# and tested with \textit{xUnit} with 217 tests and 89\% code coverage. As external packages we have used the following:
\begin{itemize}
    \item For logging, we use \textit{NLog} (v.4.5.10)
    \item \textit{dotnetRDF} (v.2.2.0) is used for RDF handling
    \item For Zip file handling we are using \textit{SharpZipLib} (v.1.0.0)
    \item File types (zip or plain text ntriples) are detected using \textit{Mime} (v.3.0.2)
    \item Json objects are handled via \textit{Newtonsoft.Json} (v.12.0.2)
    \item Sorting of filtered and inverse triples files is done using \textit{gzip} (v.1.9)
\end{itemize}

\section{Index}

In this section we present metrics for both pre-processing and indexing.

The pre-processing gave us the following results:

\begin{table}[h!]
\centering
\begin{tabular}{ll}
Task                                    & Value                     \\
\hline
Size of input file                      & 41,285,029 [KBytes]       \\
Number of input triples (lines)         & 5,126,548,635             \\
Size of output file                     & 9,184,248 [KBytes]        \\
Number of output triples                & 1,098,606,588             \\
Runtime duration                        & 34:59:22 [hh:mm:ss]       \\
\end{tabular}
\caption{Document and field representation of example triples}
\label{table:preprocessingMetrics}
\end{table}

The indexing process has the following metrics:

\begin{table}[h!]
\centering
\begin{tabular}{ll}
Task                                    & Value                      \\
\hline
Number of input triples                 & 1,098,606,588              \\
PageRank runtime duration               & 11:16:25 [hh:mm:ss]        \\
Entities index runtime duration         & 58:10:11 [hh:mm:ss]        \\
Number of entities                      & 84,623,017                 \\
Size of entities index                  & 9,721,912 [KBytes]         \\
Properties index runtime duration       & 123 [hh:mm:ss]             \\
Number of properties                    & 123                        \\
Size of properties index                & 123 KBytes                 \\
Total index size                        & 123 KBytes                 \\
\end{tabular}
\caption{Document and field representation of example triples}
\label{table:indexMetrics}
\end{table}

% There are some observations that we would like to highlight about our indexing process. While some of these observations are mostly related to the input dataset.

% Que wea?

\section{Queries}

We have mentioned before that our system will provide over-approximated suggestions. Readers might wonder how can we measure this? In this section, we would like to show how approximated our results are compared to the exact ones provided from the Wikidata Endpoint. We will use precision and recall to measure how similar our approximated results are to the exact Wikidata results.

For this, we are going to create a list of sample properties and run queries using these. We expect to get an over-approximation of our suggested values, while not leaving any values outside. In other words, we are expecting that all of our values are true positives while also having a high amount of false positives and a low-to-zero false negatives: We expect our model to have a 100\% of recall.

\subsection{Sample queries}
For our queries, we have created a list of sample properties. Selecting a sample set of properties can be a daunting task: What makes a property a good candidate for testing? 

We started by looking at some common user queries and Wikidata sample queries. Those examples are not good candidates for several reasons: First of all, the queries are built by users with both SPARQL knowledge and some Wikidata structure knowledge: Users know what they are querying for and there is no exploration task involved. Second, these queries usually do turn good results and do not time out. Lastly, the sample and user queries had very common properties between them and did not presented a distributed sample of properties. 

Based on these arguments, we decided to create a sample set based on two approaches: Random and cheery pick. Since picking properties randomly could return some not-so-valuable properties (low frequency, domain or range), we followed the following approach: First we sorted the properties randomly; we calculated the total sum of the value (e.g.: Sum of all frequencies) that we were looking for and calculated the cumulative contribution to the total. From that total, we took 10 random values between 0-1: This would give us both properties that have mixed contributions, but certainly significant. This give us 30 properties: 10 for each frequency, domain and range.

For the cherry pick, we took an additional 37 properties that had a mix of frequency/domain/range values. We manually selected some properties that had a values in between the spectrum of values and added them to the previous list, e.g.: We picked P21 (sex or gender) that has a high frequency, a high domain and a low range. We also picked P301 (category's main topic, that has a high frequency, a low domain and a high range.

We took 67 sample properties in total for our tests. In \autoref{table:sampleProperties} we present some of these. In this table can observed that there is a mix of high and low values for all frequency, domain and range.

\begin{table}[]
\centering
\begin{tabular}{lrrrl}
\hline
 
\multicolumn{1}{l}{\textbf{Prop}} & \multicolumn{1}{l}{\textbf{Label}}            & \multicolumn{1}{l}{\textbf{Freq}} & \multicolumn{1}{l}{\textbf{Domain}} & \textbf{Range} \\ 
\hline
P17     & \multicolumn{1}{l}{country}                     & 12418513  & 37642  & 664     \\
P19     & \multicolumn{1}{l}{place of birth}              & 2460395   & 938    & 3344    \\
P21     & \multicolumn{1}{l}{sex or gender}               & 6053686   & 2341   & 13      \\
P22     & \multicolumn{1}{l}{father}                      & 786266    & 483    & 399     \\
P31     & \multicolumn{1}{l}{instance of}                 & 82929158  & 70082  & 5470    \\
P36     & \multicolumn{1}{l}{capital}                     & 104973    & 1518   & 933     \\
P39     & \multicolumn{1}{l}{position held}               & 591049    & 371    & 1420    \\
P102    & \multicolumn{1}{l}{member of political party}   & 380027    & 116    & 245     \\
P301    & \multicolumn{1}{l}{category's main topic}       & 490433    & 37     & 11788   \\
P531    & \multicolumn{1}{l}{diplomatic mission sent}     & 13        & 1      & 17      \\
P807    & \multicolumn{1}{l}{separated from}              & 120       & 64     & 66      \\
P1372   & \multicolumn{1}{l}{binding of software library} & 16        & 15     & 34      \\
\end{tabular}
\caption{Sample properties}
\label{table:sampleProperties}
\end{table}

With these properties, we will build four type of queries. For each of our properties, we will consider an incoming or outgoing edge from our source or target nodes. We will represent each one of these queries by \texttt{A}, \texttt{B}, \texttt{C} and \texttt{D}, as per the following queries: 

\begin{verbatim}
A: v1 p1 v2 .      B: v1 p1 v2 .     C: v1 p1 v2 .     D: v1 p1 v2 .
   v1 p2 v3 .         v2 p2 v3 .        v3 p2 v1 .        v3 p2 v2 .
\end{verbatim}

In each of these four cases, we are going to replace \texttt{p1} with our sample property and query for the values of \texttt{p2}. This will give us a total of 268 test queries. We will run these queries on both our local index and the remote endpoint without an internal timeout (but the 50 seconds from the Wikidata API). We will record both the query execution time on both indexes as well as the returned results for them both. In the next section we will go into the details of the query execution times to finalize the chapter with a comparison of the results from both indexes.

\subsection{Query runtime}

In our previous section we described how we created a collection of 268 sample tests: 4 query types for 67 properties. In this section we are comparing the query runtime for both our local index and the remote endpoint.

\textbf{COMPLETE: We have excluded the network request times, while in the Wikidata Endpoint comparison, they are included.}

\subsection{Results approximations}

\textbf{COMPLETE: We have not measured how the approximation metrics evolve after the release of a new dump, or close before a new dump} release.

\section{Discussion}

We started this chapter by raising some questions about our system performance. In regards to the required time for indexing the Wikidata dump, from our results we can see that indexing is a time consuming process that can take up to XX hours to run. Considering that a new dump is generated every two weeks, our system could easily be scheduled to run after one of this dump generation tasks.

% , while gradually reducing correctness until a new dump is generated. 

We also considered the space requiredments involved for our backend. The system requirements regarding disk space allow our system to keep a running version with no additional requirements, but considering that sorting our index files requires a peak physical disk space of 300GB allocation requirements, which could now a days no longer considered a system constraint.

Comparing our times and precision in the results against normal SPARQL query times can be answered once the indexing process is done. Our system can deliver results within XX seconds with an YY\% of approximation to the correct ones. It is to our readers to evaluate if XX seconds are considered a acceptable time for user interactivity, we consider that it is within common applications response times, specially if compared against the 10+ seconds (or even timeout) that some queries might take to compute on the remote SPARQL endpoint.

Related to precision, we have something else to add. Since our specialized index is built from a dump, which is generated every two weeks, the correctness of the suggestions will gradually reduce: Some triples which establish some domain and range relations might be removed or new ones might be added in the online dataset, which in turn generate a difference between our and the SPARQL endpoint results. This might be avoided by periodically generating a new specialized index, or implementing a delta mechanism to keep the system up to date. This however is out of our current scope.

Some caveats are to be mentioned at this point. It is to be noted that we have not compared relevance in the order of the suggestions. We are implementing both an inverted index TF-IDF and PageRank relevance and importance algorithms, which fall out ouf our project scope, still from our experience, the desired results have always been displayed within the first given suggestions.
